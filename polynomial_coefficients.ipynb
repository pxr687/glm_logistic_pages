{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a777e13-4e5e-4976-ad5a-88b9f7cc07da",
   "metadata": {},
   "source": [
    "## Interpreting coefficients in polynomial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a27443-4564-4fd6-86da-2f6318db4196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyprint import jupyprint, arraytex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13b8cff-e04e-4a20-827f-bbf2f997c749",
   "metadata": {},
   "source": [
    "- the slopes of polynomial terms are easy to \"see\" the effects of graphically...\n",
    "\n",
    "- ...but trickier to interpret than \"normal\" slopes\n",
    "\n",
    "- even a single-predictor model with a quadratic term is a multiple regression model, but the \"one unit change while everything else is held constant\" interpretation *cannot* be true\n",
    "\n",
    "- if we have $x$ and $x^{2}$ in a model, as $x$ increases by 1 unit, so must $x^2$ (likewise for other polynomial terms)\n",
    "\n",
    "- the expected change in the outcome variable for a 1-unit increase in the predictor **changes depending what level of the predictor we look at* e.g. it changes at different rates at low values of the predictor vs high values of the predictor\n",
    "\n",
    "- let's investigate this further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4abacc9-5e68-4c57-8803-63ef171fccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df = pd.read_csv('data/Auto.csv')\n",
    "\n",
    "display(cars_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e4d41-c858-4119-bb25-cf244ca99cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "quadratic_mod = smf.ols('mpg ~ horsepower + I(horsepower**2)', data=cars_df).fit()\n",
    "\n",
    "quadratic_mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ba349-6995-4866-9933-e073570a1371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_unit_difference_prediction(horsepower_score, return_diff=True):\n",
    "    \"A function to calculate the expected change in the outcome variable, at different values of the predictor.\"\n",
    "    car_1_prediction = quadratic_mod.params['Intercept'] + quadratic_mod.params['horsepower']*horsepower_score + quadratic_mod.params['I(horsepower ** 2)']*horsepower_score**2\n",
    "    car_2_prediction = quadratic_mod.params['Intercept'] + quadratic_mod.params['horsepower']*(horsepower_score+1) + quadratic_mod.params['I(horsepower ** 2)']*(horsepower_score+1)**2\n",
    "    jupyprint(\"________________________________________\")\n",
    "    jupyprint(f\"Prediction for `horsepower == {horsepower_score}` is {round(car_1_prediction, 2)}\")\n",
    "    jupyprint(f\"Prediction for `horsepower == {horsepower_score+1}` is {round(car_2_prediction, 2)}\")\n",
    "    jupyprint(f\"The difference is {round(car_2_prediction - car_1_prediction, 3)}\")\n",
    "    if return_diff == True:\n",
    "        return round(car_2_prediction - car_1_prediction, 3)\n",
    "\n",
    "one_unit_difference_prediction(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc39f3-5909-4b69-9dd3-02cd3edf62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = np.array([])\n",
    "horsepower_range = np.arange(cars_df['horsepower'].min(), cars_df['horsepower'].max()+1, 10)\n",
    "for score in horsepower_range:\n",
    "    differences = np.append(differences, one_unit_difference_prediction(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aec3a3-925e-4c6a-bc2d-c22bcd64306a",
   "metadata": {},
   "source": [
    "We observe a nonconstant \"expected change in the outcome variable at for a 1-unit change in the predictor\" depending whether we are predicting for low values of the predictor or high values of the predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687db3c7-6561-4c61-84f8-3d5602dfdb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nonconstant expected change per 1-unit increase in the predictor\n",
    "plt.scatter(horsepower_range, differences)\n",
    "plt.xlabel('Horsepower')\n",
    "plt.ylabel('Difference in $\\hat{y}$ Per One-Unit Change in Horsepower');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986b1bf-0589-4914-b41c-80efd26f2296",
   "metadata": {},
   "source": [
    "If you're so inclined, you can get the same information by doing some calculus with the slopes, *for a specific value of the predictor* (see [this stats.stackexchange post for more](https://stats.stackexchange.com/a/28750)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4483a-68ab-4926-86bc-05e59083967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected 1-unit change in the outcome (compare this to the printout above, for the change between `horsepower == 226`\n",
    "# and `horsepower == 227`\n",
    "quadratic_mod.params['horsepower'] + 2*quadratic_mod.params['I(horsepower ** 2)'] * 226"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabdc36e-c797-4d8a-a569-e79a52fedae7",
   "metadata": {},
   "source": [
    "# Polynomial terms in logistic regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a812f1-1c9d-410b-973f-40a943dcdc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code ported/adapted from R to python from here: https://book.stat420.org/logistic-regression.html\n",
    "\n",
    "def simulated_quadratic_relationship(sample_size=50,\n",
    "                                     predictor_abs_max=10,\n",
    "                                     intercept = -1.5,\n",
    "                                     linear_slope = 0.5,\n",
    "                                     quadratic_slope = 0,\n",
    "                                     cubic_slope = 0,\n",
    "                                     quartic_slope = 0,\n",
    "                                     quintic_slope = 0,\n",
    "                                     sextic_slope = 0,\n",
    "                                     septic_slope =0,\n",
    "                                    show_log_odds=False):\n",
    "\n",
    "    x = np.linspace(-predictor_abs_max, predictor_abs_max )\n",
    "    x_points= np.random.uniform(-predictor_abs_max,predictor_abs_max , size = sample_size)\n",
    "    linear_predictor = intercept + linear_slope * x\n",
    "    linear_predictor_points = intercept + linear_slope * x_points\n",
    "    for i, slope in enumerate([quadratic_slope, cubic_slope, quartic_slope, quintic_slope, sextic_slope, septic_slope]):\n",
    "        linear_predictor += slope * x**(i+2)\n",
    "        linear_predictor_points += slope * x_points**(i+2)\n",
    "\n",
    "    p = np.exp(linear_predictor)/(1 + np.exp(linear_predictor))\n",
    "\n",
    "    log_odds = np.log((p)/(1-p))\n",
    "\n",
    "    p_points = np.exp(linear_predictor_points)/(1 + np.exp(linear_predictor_points))\n",
    "    actual_y = np.array([])\n",
    "    \n",
    "    for i in np.arange(sample_size):\n",
    "        actual_y = np.append(actual_y, np.random.binomial(1, p = p_points[i]))        \n",
    "\n",
    "    if show_log_odds != False:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(x, p)\n",
    "        plt.scatter(x_points, actual_y, color = 'red')\n",
    "        plt.xlabel(\"Predictor\")\n",
    "        plt.ylim([0, 1])\n",
    "        plt.ylabel(\"Probability\")\n",
    "    \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(x, log_odds)\n",
    "        plt.xlabel(\"Predictor\")\n",
    "        plt.ylabel(\"Log Odds\")\n",
    "\n",
    "    else:\n",
    "        plt.plot(x, p)\n",
    "        plt.scatter(x_points, actual_y, color = 'red')\n",
    "        plt.xlabel(\"Predictor\")\n",
    "        plt.ylim([0, 1])\n",
    "        plt.ylabel(\"Probability\")\n",
    "\n",
    "interact(simulated_quadratic_relationship, predictor_abs_max =(10, 100),\n",
    "                                     sample_size=(10, 1000, 1),\n",
    "                                     intercept = (-1, 1, 0.1),\n",
    "                                     linear_slope = (-2, 2, 0.001),\n",
    "                                     quadratic_slope = (-1, 1, 0.001),\n",
    "                                     cubic_slope = (-1, 1, 0.001),\n",
    "                                     quartic_slope = (-0.01, 0.01, 0.001),\n",
    "                                     quintic_slope = (-0.01, 0.01, 0.001),\n",
    "                                     sextic_slope = (-0.001, 0.001, 0.0001),\n",
    "                                     septic_slope =(-0.0001, 0.0001, 0.00001));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f064a7dd-fd66-451d-a505-655230c56820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
