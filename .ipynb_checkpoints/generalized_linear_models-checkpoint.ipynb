{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1ae42be",
   "metadata": {},
   "source": [
    "## Generalized Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyprint import jupyprint, arraytex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dd1d00",
   "metadata": {},
   "source": [
    "[Recall the linear model...]\n",
    "\n",
    "[Foundational statistical model in culture 1 language]\n",
    "\n",
    "[Foundation machine learning algortihm in culture 2 language]\n",
    "\n",
    "Where there are $k$ predictor variables:\n",
    "\n",
    "$ \\Large \\hat{y_i} = b_0 + b_1 x_{1i} + ... b_k x_{ki} $\n",
    "\n",
    "Let's keep it simple for now and use just one predictor (but it all generalizes to multiple predictors also):\n",
    "\n",
    "$ \\Large \\hat{y_i} = b_0 + b_1 x_{i} $\n",
    "\n",
    "Another way of writing this, using the notation of conditional probability is:\n",
    "\n",
    "$ \\Large E(y_i|x_{i}) = b_0 + b_1 x_{i} $\n",
    "\n",
    "In english this reads as: \n",
    "- \"the expected value of $y_i$, given the value of $x_i$ is equal to $b_0 + b_1 x_{i}$\" \n",
    "- ... or ...\n",
    "- \"the expected value of $y_i$ is a linear function of $x_i$ with the slope $b_1$ and the intercept $b_0$\".\n",
    "\n",
    "Let's represent \"$E(y_i|x_{1i})$\" using the symbol \"$\\hat{\\mu}_i$\" (this symbol is called \"mu\").\n",
    "\n",
    "We can now write the linear regression model as:\n",
    "\n",
    "$ \\Large \\hat{\\mu}_i = b_0 + b_1 x_{1i} $\n",
    "\n",
    "Now let's write a really boring python function, called $f()$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(mu):\n",
    "    \n",
    "    result = mu * 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5408492b",
   "metadata": {},
   "source": [
    "<b>Question: </b> just from looking at the function, which of these would be true:\n",
    "\n",
    "$ \\Large \\hat{\\mu}_i \\neq f(\\hat{\\mu}_i)$\n",
    "\n",
    "or\n",
    "\n",
    "$ \\Large \\hat{\\mu}_i == f(\\hat{\\mu}_i)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dffc5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our simulated predictor variable scores\n",
    "x = np.array([4, 6, 9, 2, 6, 7, 3, 3, 6, 7])\n",
    "\n",
    "b_0 = 1\n",
    "\n",
    "b_1 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = b_0 + b_1 * x\n",
    "\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86265cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyprint(\"$\\Large \\hat{\\mu}_i = b_0 + b_1 x_{1i} $\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df67430",
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyprint(\"$\\Large \\hat{\\mu_i} =\"+f\" {b_0} + {b_1} * {arraytex(np.atleast_2d(x).T)} $\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4d6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyprint(f\"$\\Large {arraytex(np.atleast_2d(mu).T)} = {b_0} + {b_1} * {arraytex(np.atleast_2d(x).T)} $\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd7c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97e052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all(f(mu) == mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b338ee8",
   "metadata": {},
   "source": [
    "[Let's call $f()$ the \"identity function\", because whatever input we give it, it returns the same value(s) as its output].\n",
    "\n",
    "This is the *key thing to take away from this page*: **generalized linear models are just linear models where $f()$ is some other, more complex function than the identity function**.\n",
    "\n",
    "$\\Large f(\\hat{\\mu}_i) = b_0 + b_1 x_{i} $\n",
    "\n",
    "$\\Large f(\\hat{\\mu}_i) = b_0 + b_1 x_{1i} + b_2 x_{2i} + ... b_k x_{ki}$\n",
    "\n",
    "What this allows is to use the slope/intercept framework of linear regression (and its associated interpreations) with different types of outcome variable.\n",
    "\n",
    "[E.g. linear regression is used for numeric outcome variables, it produces predictions which range from $-\\infty$ to $\\infty$]\n",
    "\n",
    "[Some outcome variables do not cover this range e.g. dummy-coded categorical variables which take values only of 0 or 1]\n",
    "\n",
    "[We can use generalized linear models with these other types of variable]\n",
    "\n",
    "[We just need a function, $f()$ which *maps* or *links* linear predictions on the $-\\infty$ to $\\infty$ scale to the actual scale of the outcome variable]\n",
    "\n",
    "[One way of thinking of these models is that there is *some scale on which the model predictions form a straight line* (or plane/hyperplane, in the case of multiple predictors].\n",
    "\n",
    "| Model type               | Range of predicted values | Example Outcome Variable  |Link name     | Link function $f(\\mu)$|\n",
    "|--------------------------|------------------------|------------------|-----------------------|-----------------------|\n",
    "| Linear Regression        | (-∞, ∞)                | Income           |Identity       | 1 * $μ$               |\n",
    "| Logistic Regression      | {0, 1}                 | Died/Survived    |Logit            | $ln(\\frac{μ} {1 – μ})$|\n",
    "| Poisson Regression       | 0, 1, 2, …  int(∞)     | Number of children|Log              | $ln(μ) $              |\n",
    "| Gamma Regression         | (0, ∞)                 | Waiting times     |Inverse          | $1/μ $                |"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
